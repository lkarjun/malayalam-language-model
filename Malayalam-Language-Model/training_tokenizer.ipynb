{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkarjun/malayalam-language-model/blob/language-model/training_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LycIPSbAG9Lx"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF6bu7JFHCmI"
      },
      "outputs": [],
      "source": [
        "!pip install -qq dvc[gdrive]\n",
        "\n",
        "!dvc get https://github.com/lkarjun/malayalam-language-model \\\n",
        "Datasets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TAuuV3ePHWwF"
      },
      "outputs": [],
      "source": [
        "!unzip -q 'Datasets/*.zip' -d Datasets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vv0G6VcPIM73"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DS_Path = Path(\"../Datasets\")\n",
        "\n",
        "csv_files = [\"magazine_files.csv\", \n",
        "             \"wikitext_files.csv\",\n",
        "             \"article_files.csv\"\n",
        "             ]\n",
        "             \n",
        "df = pd.concat([pd.read_csv(DS_Path/csv) for csv in csv_files])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WS53797YMJFK"
      },
      "outputs": [],
      "source": [
        "sample = df['file_path'][:10].to_list()\n",
        "\n",
        "with open(sample[0], \"r\", encoding=\"utf-8\") as file:\n",
        "  sample_txt = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZGPsto2Fw1J"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmSR2POgF1p4",
        "outputId": "09c69116-51fa-44f4-a362-d6d4976e7650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 77 kB 3.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 32.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 51.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq tokenizer transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8c2eXgNjG5ig"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer, pre_tokenizers, decoders\n",
        "from tokenizers.models import Unigram, WordPiece\n",
        "from tokenizers.trainers import UnigramTrainer, WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers.pre_tokenizers import Digits\n",
        "from tokenizers.normalizers import Strip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vXQIc0YBVlsr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVECNzyJFn-S"
      },
      "source": [
        "## Training Subword Tokenizer For Malayalm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DpL6YNUjFjKs"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Strip()\n",
        "tokenizer.decoders = decoders.WordPiece()\n",
        "\n",
        "trainer = WordPieceTrainer(vocab_size=1000,\n",
        "                           min_frequency=4,\n",
        "                           special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                           show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dVmDN5XiR0sm"
      },
      "outputs": [],
      "source": [
        "pre_tokenizer = pre_tokenizers.Sequence([\n",
        "                                         Whitespace(),  \n",
        "                                         Digits(individual_digits=False)\n",
        "                                        ])\n",
        "\n",
        "\n",
        "tokenizer.pre_tokenizer = pre_tokenizer\n",
        "\n",
        "# training tokenizer\n",
        "tokenizer.train(sample, trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "E6fu_gOtJb6m"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = TemplateProcessing(\n",
        "                                single=\"[CLS] $A [SEP]\",\n",
        "                                pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "                                special_tokens=[\n",
        "                                        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "                                        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "                                  ],\n",
        "                              )\n",
        "tokenizer.decoders = decoders.WordPiece()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "G-X0zbwuvO-a"
      },
      "outputs": [],
      "source": [
        "tokenizer.enable_padding(direction=\"right\", pad_id=3, pad_token=\"[PAD]\")\n",
        "tokenizer.enable_truncation(max_length=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CUNIXlrytIS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAi-ASpULNPU",
        "outputId": "1b34047c-da21-4d4a-8f58-3c3304725b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'ഇ', '##ന്ത്', '##യ', '##യിലെ', 'ആദ്യ', '##ത്തെ', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "output = tokenizer.encode(sample_txt[:20])\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQGg6IdjLex3",
        "outputId": "975a5895-3823-4ad9-b6d1-e38bdf622b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 33, 862, 108, 596, 584, 286, 2]\n"
          ]
        }
      ],
      "source": [
        "print(output.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Rp5OVuewZQMG",
        "outputId": "21d14cd0-aa83-4242-c6a9-89f86958c1c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ഇ ##ന്ത് ##യ ##യിലെ ആദ്യ ##ത്തെ'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(output.ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSGO2O6OYXEI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydBwuUIpPsdc",
        "outputId": "de5431ea-8da7-4bfa-c59d-c9efbd4781eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['[CLS]', '[UNK]', '[SEP]'], [1, 0, 2], '[UNK]')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "check_unk = tokenizer.encode(\"Hello\")\n",
        "check_unk.tokens, check_unk.ids, tokenizer.id_to_token(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRVaTiy9tV5l",
        "outputId": "f657b319-286b-418a-c4b8-0390d405d694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'ഇ', '##ന്ത്', '##യ', '##യിലെ', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "['[CLS]', 'ഇ', '##ന്ത്', '##യ', '##യിലെ', 'ആദ്യ', '##ത്തെ', 'വ', '##നി', '##ത', '##ാ', 'ഐ', '##\\u200c', '##എ', '##\\u200c', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "output = tokenizer.encode_batch([sample_txt[:30], sample_txt[:10]])\n",
        "print(output[1].tokens)\n",
        "print(output[0].tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU5WPi5hw4Tx",
        "outputId": "0096d012-1e64-45b5-8352-0bcfb5b0a734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(output[1].attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R0t-KcUZ1Ko"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_JZj_ctb8Ud"
      },
      "source": [
        "### Training Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Em_n5aU8YHFu"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Strip()\n",
        "tokenizer.decoders = decoders.WordPiece()\n",
        "\n",
        "trainer = WordPieceTrainer(vocab_size=75000,\n",
        "                           min_frequency=4,\n",
        "                           special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                           show_progress=True)\n",
        "\n",
        "pre_tokenizer = pre_tokenizers.Sequence([\n",
        "                                         Whitespace(),  \n",
        "                                         Digits(individual_digits=False)\n",
        "                                        ])\n",
        "\n",
        "\n",
        "tokenizer.pre_tokenizer = pre_tokenizer\n",
        "\n",
        "# training tokenizer\n",
        "tokenizer.train(df['file_path'], trainer)\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "                                single=\"[CLS] $A [SEP]\",\n",
        "                                pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "                                special_tokens=[\n",
        "                                        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "                                        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "                                  ],\n",
        "                              )\n",
        "\n",
        "tokenizer.decoders = decoders.WordPiece()\n",
        "\n",
        "tokenizer.enable_padding(direction=\"right\", pad_id=3, pad_token=\"[PAD]\")\n",
        "tokenizer.enable_truncation(max_length=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "B0Xcm_KIYHAr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "FatmynqpwHgO"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"tokenizer-malayalam.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PbooHwFy6Ko"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zmU-iyOKmPOP"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "6_bFvuorAPDc"
      },
      "outputs": [],
      "source": [
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 1 2\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    wrapped_tokenizer.pad_token_id,\n",
        "    wrapped_tokenizer.cls_token_id,\n",
        "    wrapped_tokenizer.sep_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BFlFJBnAmk1",
        "outputId": "dabe9b2c-f742-40a5-e300-7e0f26d36241"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Tokenizer\\\\tokenizer_config.json',\n",
              " 'Tokenizer\\\\special_tokens_map.json',\n",
              " 'Tokenizer\\\\tokenizer.json')"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wrapped_tokenizer.save_pretrained(\"Tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMyAaExI1FufC5MoQtC+joB",
      "include_colab_link": true,
      "name": "training-tokenizer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
