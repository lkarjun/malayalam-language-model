{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training-tokenizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyAaExI1FufC5MoQtC+joB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkarjun/malayalam-language-model/blob/language-model/training_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "LycIPSbAG9Lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq dvc[gdrive]\n",
        "\n",
        "!dvc get https://github.com/lkarjun/malayalam-language-model \\\n",
        "Datasets/"
      ],
      "metadata": {
        "id": "cF6bu7JFHCmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q 'Datasets/*.zip' -d Datasets/"
      ],
      "metadata": {
        "id": "TAuuV3ePHWwF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DS_Path = Path(\"/content/Datasets\")\n",
        "\n",
        "csv_files = [\"magazine_files.csv\", \n",
        "             \"wikitext_files.csv\",\n",
        "             \"article_files.csv\"\n",
        "             ]\n",
        "             \n",
        "df = pd.concat([pd.read_csv(DS_Path/csv) for csv in csv_files])"
      ],
      "metadata": {
        "id": "Vv0G6VcPIM73"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = df['file_path'][:10].to_list()\n",
        "\n",
        "with open(sample[0], \"r\") as file:\n",
        "  sample_txt = file.read()"
      ],
      "metadata": {
        "id": "WS53797YMJFK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "wZGPsto2Fw1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq tokenizer transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmSR2POgF1p4",
        "outputId": "09c69116-51fa-44f4-a362-d6d4976e7650"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 77 kB 3.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 32.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 51.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, pre_tokenizers, decoders\n",
        "from tokenizers.models import Unigram, WordPiece\n",
        "from tokenizers.trainers import UnigramTrainer, WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers.pre_tokenizers import Digits\n",
        "from tokenizers.normalizers import Strip"
      ],
      "metadata": {
        "id": "8c2eXgNjG5ig"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vXQIc0YBVlsr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Subword Tokenizer For Malayalm "
      ],
      "metadata": {
        "id": "iVECNzyJFn-S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DpL6YNUjFjKs"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(WordPiece(unk_token=\"<unk>\"))\n",
        "tokenizer.normalizer = Strip()\n",
        "tokenizer.decoders = decoders.WordPiece()\n",
        "\n",
        "trainer = WordPieceTrainer(vocab_size=1000,\n",
        "                           min_frequency=4,\n",
        "                           special_tokens=[\"<unk>\", \"<bos>\", \"<eos>\", \"<pad>\", \"<mask>\"],\n",
        "                           show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_tokenizer = pre_tokenizers.Sequence([\n",
        "                                         Whitespace(),  \n",
        "                                         Digits(individual_digits=False)\n",
        "                                        ])\n",
        "\n",
        "\n",
        "tokenizer.pre_tokenizer = pre_tokenizer\n",
        "\n",
        "# training tokenizer\n",
        "tokenizer.train(sample, trainer)"
      ],
      "metadata": {
        "id": "dVmDN5XiR0sm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.post_processor = TemplateProcessing(\n",
        "                                single=\"<bos> $A <eos>\",\n",
        "                                pair=\"<bos> $A <eos> <bos> $B:1 <eos>:1\",\n",
        "                                special_tokens=[\n",
        "                                        (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
        "                                        (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
        "                                  ],\n",
        "                              )\n",
        "tokenizer.decoders = decoders.WordPiece()"
      ],
      "metadata": {
        "id": "E6fu_gOtJb6m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.enable_padding(pad_id=3, pad_token=\"<pad>\")\n",
        "tokenizer.enable_truncation(max_length=500)"
      ],
      "metadata": {
        "id": "G-X0zbwuvO-a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5CUNIXlrytIS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(sample_txt[:20])\n",
        "print(output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAi-ASpULNPU",
        "outputId": "1b34047c-da21-4d4a-8f58-3c3304725b47"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<bos>', 'ഇ', '##ന്ത്', '##യ', '##യിലെ', 'ആദ്യ', '##ത്തെ', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQGg6IdjLex3",
        "outputId": "975a5895-3823-4ad9-b6d1-e38bdf622b68"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 33, 861, 117, 596, 583, 287, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output.ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Rp5OVuewZQMG",
        "outputId": "21d14cd0-aa83-4242-c6a9-89f86958c1c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ഇ ##ന്ത് ##യ ##യിലെ ആദ്യ ##ത്തെ'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TSGO2O6OYXEI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_unk = tokenizer.encode(\"Hello\")\n",
        "check_unk.tokens, check_unk.ids, tokenizer.id_to_token(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydBwuUIpPsdc",
        "outputId": "de5431ea-8da7-4bfa-c59d-c9efbd4781eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<bos>', '<unk>', '<eos>'], [1, 0, 2], '<unk>')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode_batch([sample_txt[:30], sample_txt[:10]])\n",
        "print(output[1].tokens)\n",
        "print(output[0].tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRVaTiy9tV5l",
        "outputId": "f657b319-286b-418a-c4b8-0390d405d694"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<bos>', 'ഇ', '##ന്ത്', '##യ', '##യിലെ', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "['<bos>', 'ഇ', '##ന്ത്', '##യ', '##യിലെ', 'ആദ്യ', '##ത്തെ', 'വ', '##നി', '##ത', '##ാ', 'ഐ', '##\\u200c', '##എ', '##\\u200c', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[1].attention_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU5WPi5hw4Tx",
        "outputId": "0096d012-1e64-45b5-8352-0bcfb5b0a734"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1R0t-KcUZ1Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Tokenizer"
      ],
      "metadata": {
        "id": "m_JZj_ctb8Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(WordPiece(unk_token=\"<unk>\"))\n",
        "tokenizer.normalizer = Strip()\n",
        "tokenizer.decoders = decoders.WordPiece()\n",
        "\n",
        "trainer = WordPieceTrainer(vocab_size=100000,\n",
        "                           min_frequency=4,\n",
        "                           special_tokens=[\"<unk>\", \"<bos>\", \"<eos>\", \"<pad>\", \"<mask>\"],\n",
        "                           show_progress=True)\n",
        "\n",
        "pre_tokenizer = pre_tokenizers.Sequence([\n",
        "                                         Whitespace(),  \n",
        "                                         Digits(individual_digits=False)\n",
        "                                        ])\n",
        "\n",
        "\n",
        "tokenizer.pre_tokenizer = pre_tokenizer\n",
        "\n",
        "# training tokenizer\n",
        "tokenizer.train(df['file_path'], trainer)\n",
        "\n",
        "tokenizer.enable_padding(pad_id=3, pad_token=\"<pad>\")\n",
        "tokenizer.enable_truncation(max_length=500)\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "                                single=\"<bos> $A <eos>\",\n",
        "                                pair=\"<bos> $A <eos> <bos> $B:1 <eos>:1\",\n",
        "                                special_tokens=[\n",
        "                                        (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
        "                                        (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
        "                                  ],\n",
        "                              )\n",
        "\n",
        "tokenizer.decoders = decoders.WordPiece()"
      ],
      "metadata": {
        "id": "Em_n5aU8YHFu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B0Xcm_KIYHAr"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"tokenizer-malayalam.json\")"
      ],
      "metadata": {
        "id": "FatmynqpwHgO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3PbooHwFy6Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast"
      ],
      "metadata": {
        "id": "zmU-iyOKmPOP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=\"Tokenizer/tokenizer.json\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    eos_token=\"<bos>\",\n",
        "    bos_token=\"<eos>\",\n",
        "    mask_token=\"<mask>\",\n",
        ")"
      ],
      "metadata": {
        "id": "6_bFvuorAPDc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_tokenizer.save_pretrained(\"Tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BFlFJBnAmk1",
        "outputId": "dabe9b2c-f742-40a5-e300-7e0f26d36241"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Tokenizer/tokenizer_config.json',\n",
              " 'Tokenizer/special_tokens_map.json',\n",
              " 'Tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}